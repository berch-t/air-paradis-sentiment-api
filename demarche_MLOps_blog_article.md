# De la Recherche Ã  la Production : Une Approche MLOps ComplÃ¨te pour l'Analyse de Sentiment

## Introduction

Dans le cadre d'une mission de conseil pour la compagnie aÃ©rienne fictive "Air Paradis", nous avons dÃ©veloppÃ© un systÃ¨me complet d'analyse de sentiment des tweets utilisant des techniques de Deep Learning avancÃ©es et une dÃ©marche MLOps moderne. Ce projet illustre la transition d'un modÃ¨le de recherche vers une API de production robuste, dÃ©ployÃ©e sur Google Cloud avec un pipeline CI/CD automatisÃ©.

## Vue d'ensemble du projet

**Objectif :** CrÃ©er un systÃ¨me d'IA permettant d'anticiper les bad buzz sur les rÃ©seaux sociaux en analysant le sentiment des tweets en temps rÃ©el.

**Enjeux techniques :**
- Comparaison de plusieurs approches de modÃ©lisation (classique vs avancÃ© vs BERT)
- Mise en Å“uvre d'une dÃ©marche MLOps complÃ¨te
- DÃ©ploiement en production avec monitoring et alertes
- Gestion efficace des modÃ¨les volumineux

## Architecture technique

### Stack technologique
- **ModÃ©lisation :** TensorFlow 2.16, Keras, scikit-learn
- **API :** FastAPI avec Pydantic pour la validation
- **Containerisation :** Docker
- **Cloud :** Google Cloud Run, Artifact Registry, Cloud Storage
- **CI/CD :** GitHub Actions
- **Monitoring :** Google Cloud Monitoring, MLflow
- **Gestion des modÃ¨les :** Google Cloud Storage (solution innovante)

### Architecture de dÃ©ploiement

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GitHub Repo   â”‚â”€â”€â”€â–¶â”‚  GitHub Actions  â”‚â”€â”€â”€â–¶â”‚  Artifact Reg.  â”‚
â”‚   (Code Source) â”‚    â”‚   (CI/CD Pipeline)â”‚    â”‚  (Docker Images)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚ Cloud Storage   â”‚â”€â”€â”€â–¶â”‚   Cloud Run     â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ (ModÃ¨les ML)    â”‚    â”‚  (API Production)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ Cloud Monitoring â”‚
                       â”‚    (Alertes)     â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Comparaison des approches de modÃ©lisation

### 1. ModÃ¨le sur mesure simple

**Approche :** RÃ©gression logistique avec TF-IDF
- **Avantages :** Rapide, interprÃ©table, baseline solide
- **Performance :** Accuracy ~82%, temps d'entraÃ®nement <5 min
- **Cas d'usage :** Prototypage rapide, contraintes de ressources

### 2. ModÃ¨le sur mesure avancÃ© (retenu pour la production)

**Approche :** BiLSTM bidirectionnel avec Word2Vec
- **Architecture :**
  ```python
  Embedding(Word2Vec 300d) â†’ SpatialDropout1D â†’ 
  Bidirectional(LSTM 128) â†’ Dense(64) â†’ Dense(1, sigmoid)
  ```
- **Performance :** Accuracy ~87%, F1-score 0.89
- **Avantages :** Ã‰quilibre performance/complexitÃ© optimal
- **Temps d'infÃ©rence :** ~50ms par prÃ©diction

### 3. ModÃ¨le BERT avancÃ©

**Approche :** TFBertForSequenceClassification fine-tunÃ©
- **Performance :** Accuracy ~91%, F1-score 0.92
- **InconvÃ©nients :** Temps d'infÃ©rence ~300ms, ressources importantes
- **Conclusion :** Excellent pour la recherche, trop lourd pour la production

## DÃ©marche MLOps mise en Å“uvre

### 1. ExpÃ©rimentation et tracking avec MLflow

```python
with mlflow.start_run(run_name=f"{architecture}_{embedding_type}"):
    # Enregistrement des paramÃ¨tres
    mlflow.log_param("embedding_type", embedding_type)
    mlflow.log_param("architecture", architecture)
    
    # EntraÃ®nement du modÃ¨le
    history = model.fit(X_train, y_train, validation_data=(X_val, y_val))
    
    # MÃ©triques de performance
    mlflow.log_metric("val_accuracy", val_accuracy)
    mlflow.log_metric("f1", f1_score)
    
    # Sauvegarde du modÃ¨le
    mlflow.keras.log_model(model, f"{architecture}_{embedding_type}")
```

### 2. Gestion des modÃ¨les : Innovation Google Cloud Storage

**ProblÃ¨me rencontrÃ© :** Limitation du budget Git LFS (1GB/mois) pour des modÃ¨les de 350MB+

**Solution innovante :** Migration vers Google Cloud Storage
```python
async def _download_models_if_needed(self):
    """TÃ©lÃ©charge les modÃ¨les depuis Google Cloud Storage si nÃ©cessaire"""
    bucket_url = "https://storage.googleapis.com/air-paradis-models"
    models_to_download = [
        ("best_advanced_model_BiLSTM_Word2Vec.h5", self.model_path),
        ("best_advanced_model_tokenizer.pickle", self.tokenizer_path),
        ("best_advanced_model_config.pickle", self.config_path)
    ]
    
    for filename, local_path in models_to_download:
        if not os.path.exists(local_path) or os.path.getsize(local_path) < 1000:
            logger.info(f"ğŸ’¾ TÃ©lÃ©chargement de {filename} depuis GCS...")
            url = f"{bucket_url}/{filename}"
            urllib.request.urlretrieve(url, local_path)
```

**Avantages de cette approche :**
- âœ… Gratuit (25GB/mois avec GCS)
- âœ… Rapide (tÃ©lÃ©chargement en ~10 secondes)
- âœ… Scalable et professionnel
- âœ… SÃ©paration claire code/modÃ¨les

### 3. Pipeline CI/CD avec GitHub Actions

```yaml
jobs:
  test:
    name: Run Tests
    steps:
    - name: Run unit tests
      run: python -m pytest tests/ -v --tb=short
    
  build:
    name: Build Docker Image
    needs: test
    steps:
    - name: Build and push to Artifact Registry
      run: |
        docker build -t $IMAGE_TAG .
        docker push $IMAGE_TAG
    
  deploy:
    name: Deploy to Cloud Run
    needs: build
    steps:
    - name: Deploy to production
      run: |
        gcloud run deploy $SERVICE_NAME \
          --image $IMAGE_TAG \
          --memory 2Gi --cpu 1 --timeout 600
```

### 4. API de production avec FastAPI

```python
@app.post("/predict", response_model=SentimentResponse)
async def predict_sentiment(request: SentimentRequest):
    try:
        # PrÃ©diction avec gestion des erreurs
        result = await model_manager.predict(request.text)
        
        # Tracking MLflow (dÃ©sactivÃ© en production)
        if MLFLOW_AVAILABLE:
            with mlflow.start_run():
                mlflow.log_metric("confidence", result["confidence"])
        
        return SentimentResponse(**result)
    except Exception as e:
        # Monitoring des erreurs
        background_tasks.add_task(send_error_to_monitoring, str(e), request.text)
        raise HTTPException(status_code=500, detail="Erreur interne")
```

## Monitoring et alertes en production

### 1. MÃ©triques surveillÃ©es

- **Performance :** Temps de rÃ©ponse, throughput, taux d'erreur
- **QualitÃ© :** Feedback utilisateur, accuracy en production
- **Infrastructure :** Utilisation mÃ©moire/CPU, santÃ© des conteneurs

### 2. SystÃ¨me d'alertes Google Cloud

```python
async def send_error_to_monitoring(error_message: str, text: str):
    """Envoie une erreur au monitoring Google Cloud"""
    global error_count, last_errors
    error_count += 1
    
    # Garder les erreurs des 5 derniÃ¨res minutes
    current_time = datetime.utcnow()
    five_minutes_ago = current_time.timestamp() - 300
    last_errors = [err for err in last_errors 
                   if err["timestamp"].timestamp() > five_minutes_ago]
    
    # DÃ©clencher une alerte si 3 erreurs en 5 minutes
    if len(last_errors) >= 3:
        await send_alert_email()
```

### 3. Interface de feedback utilisateur

```python
@app.post("/feedback")
async def submit_feedback(feedback: FeedbackRequest):
    """Soumission de feedback utilisateur pour l'amÃ©lioration continue"""
    if not feedback.is_correct:
        # Envoyer au monitoring pour analyse
        background_tasks.add_task(
            send_error_to_monitoring, 
            f"PrÃ©diction incorrecte: {feedback.predicted_sentiment} vs {feedback.actual_sentiment}",
            feedback.text
        )
    
    # Tracking pour amÃ©lioration future du modÃ¨le
    if MLFLOW_AVAILABLE:
        with mlflow.start_run():
            mlflow.log_metric("is_correct", 1 if feedback.is_correct else 0)
```

## Tests et qualitÃ© du code

### 1. Tests unitaires automatisÃ©s

```python
@pytest.mark.asyncio
async def test_predict_sentiment():
    """Test de prÃ©diction de sentiment"""
    async with AsyncClient(app=app, base_url="http://test") as ac:
        response = await ac.post("/predict", json={
            "text": "I love this airline!"
        })
    
    assert response.status_code == 200
    data = response.json()
    assert data["sentiment"] in ["positive", "negative"]
    assert 0 <= data["confidence"] <= 1
```

### 2. Tests d'intÃ©gration

```python
def test_health_check():
    """Test de santÃ© de l'API"""
    response = client.get("/health")
    assert response.status_code == 200
    assert response.json()["status"] == "healthy"
```

## RÃ©sultats et mÃ©triques

### Performance des modÃ¨les
| ModÃ¨le | Accuracy | F1-Score | Temps d'infÃ©rence | Taille |
|--------|----------|----------|-------------------|---------|
| RÃ©gression Logistique | 82% | 0.81 | 5ms | 2MB |
| BiLSTM + Word2Vec | 87% | 0.89 | 50ms | 350MB |
| BERT Fine-tunÃ© | 91% | 0.92 | 300ms | 500MB |

### MÃ©triques de production
- **DisponibilitÃ© :** 99.9% (SLA Google Cloud Run)
- **Temps de rÃ©ponse moyen :** 120ms (incluant tÃ©lÃ©chargement modÃ¨les)
- **Throughput :** 100 req/sec maximum
- **CoÃ»t mensuel :** ~15â‚¬ (Cloud Run + Storage + Monitoring)

## DÃ©fis techniques rencontrÃ©s et solutions

### 1. CompatibilitÃ© TensorFlow 2.16 avec tokenizers legacy

**ProblÃ¨me :** Erreurs `keras.src.legacy` avec anciens tokenizers

**Solution :** CrÃ©ation d'un CompatibleTokenizer
```python
class CompatibleTokenizer:
    """Tokenizer compatible crÃ©Ã© Ã  partir d'un word_index extractÃ©"""
    
    def __init__(self, word_index, num_words=None):
        self.word_index = word_index
        self.num_words = num_words or len(word_index)
        self.index_word = {v: k for k, v in word_index.items()}
    
    def texts_to_sequences(self, texts):
        sequences = []
        for text in texts:
            words = text.lower().split()
            sequence = [self.word_index.get(word, 0) for word in words]
            sequences.append(sequence)
        return sequences
```

### 2. Limitation budget Git LFS

**ProblÃ¨me :** DÃ©passement du budget 1GB/mois GitHub LFS

**Solution :** Migration vers Google Cloud Storage
- CoÃ»t : 0â‚¬ (dans les limites gratuites)
- Performance : Ã‰quivalente Ã  Git LFS
- ScalabilitÃ© : IllimitÃ©e

### 3. Timeout de dÃ©marrage Cloud Run

**ProblÃ¨me :** Chargement des modÃ¨les trop long (>240s)

**Solutions :**
- Augmentation timeout Ã  600s
- Optimisation chargement avec `tf.keras.backend.clear_session()`
- TÃ©lÃ©chargement asynchrone des modÃ¨les

## AmÃ©liorations futures et roadmap

### Court terme (1-3 mois)
1. **A/B Testing :** Comparaison de modÃ¨les en production
2. **Auto-retraining :** Pipeline d'entraÃ®nement automatique avec nouveaux donnÃ©es
3. **Cache Redis :** Mise en cache des prÃ©dictions frÃ©quentes

### Moyen terme (3-6 mois)
1. **ModÃ¨le ensembliste :** Combinaison BiLSTM + BERT pour performances optimales
2. **Multi-langues :** Support franÃ§ais, espagnol, allemand
3. **Streaming :** Traitement en temps rÃ©el avec Apache Kafka

### Long terme (6-12 mois)
1. **Edge deployment :** DÃ©ploiement sur CDN pour latence minimale
2. **AutoML :** Optimisation automatique d'hyperparamÃ¨tres
3. **ExplainabilitÃ© :** LIME/SHAP pour expliquer les prÃ©dictions

## Bonnes pratiques MLOps identifiÃ©es

### 1. SÃ©paration des responsabilitÃ©s
- **Code :** Git classique avec versioning sÃ©mantique
- **ModÃ¨les :** Cloud Storage avec versioning automatique
- **Config :** Variables d'environnement et secrets managÃ©s

### 2. ObservabilitÃ© multicouche
- **Applicatif :** Logs structurÃ©s, mÃ©triques custom
- **Infrastructure :** Monitoring Cloud Run natif
- **MÃ©tier :** Feedback utilisateur, accuracy en production

### 3. DÃ©ploiement progressif
- **Tests automatisÃ©s :** Unitaires + intÃ©gration + e2e
- **Environnements :** Dev â†’ Staging â†’ Production
- **Rollback automatique :** En cas de dÃ©gradation des mÃ©triques

## Retour d'expÃ©rience et lessons learned

### Ce qui a bien fonctionnÃ© âœ…
1. **Google Cloud Storage pour modÃ¨les :** Solution Ã©lÃ©gante au problÃ¨me Git LFS
2. **FastAPI + Pydantic :** Validation automatique et documentation swagger
3. **GitHub Actions :** Pipeline CI/CD simple et efficace
4. **Monitoring proactif :** DÃ©tection rapide des problÃ¨mes

### DÃ©fis rencontrÃ©s âš ï¸
1. **CompatibilitÃ© TensorFlow :** Versions et dÃ©pendances complexes
2. **Gestion des timeouts :** Ã‰quilibre entre robustesse et rÃ©activitÃ©
3. **CoÃ»ts Cloud :** Surveillance nÃ©cessaire pour Ã©viter les dÃ©rives

### Recommandations pour projets similaires ğŸ’¡
1. **Commencer simple :** MVP avec modÃ¨le lÃ©ger, complexifier progressivement
2. **Tester tÃ´t et souvent :** Tests automatisÃ©s dÃ¨s le dÃ©but du projet
3. **Monitorer tout :** Logs, mÃ©triques, feedback utilisateur
4. **Documenter les choix :** DÃ©cisions techniques et compromis effectuÃ©s

## Conclusion

Ce projet illustre une approche complÃ¨te de MLOps, de la recherche Ã  la production, en gÃ©rant les contraintes techniques rÃ©elles (budget, performance, compatibilitÃ©). La migration des modÃ¨les vers Google Cloud Storage reprÃ©sente une innovation pragmatique qui pourrait inspirer d'autres projets confrontÃ©s aux limitations de Git LFS.

L'API dÃ©ployÃ©e dÃ©montre qu'il est possible de mettre en production des modÃ¨les de Deep Learning performants avec un budget limitÃ© (~15â‚¬/mois) tout en maintenant des standards de qualitÃ© professionnels.

Le code source complet et la documentation technique sont disponibles sur GitHub : [air-paradis-sentiment-api](https://github.com/berch-t/air-paradis-sentiment-api)

**API en production :** https://air-paradis-sentiment-api-qxumenjqxq-ew.a.run.app

---

*Article rÃ©digÃ© dans le cadre du projet "RÃ©aliser une analyse de sentiments grÃ¢ce au Deep Learning" - Parcours IngÃ©nieur IA, OpenClassrooms.*
