"""
Air Paradis Sentiment Analysis API
API FastAPI pour l'analyse de sentiment des tweets
Mod√®le: BiLSTM avec Word2Vec (compatible TensorFlow 2.13 + gestion tokenizers incompatibles)
"""

import os
import logging
import pickle
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict, List, Optional
import asyncio
from contextlib import asynccontextmanager

import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse

# Pydantic v1 pour compatibilit√© avec TensorFlow 2.13
from pydantic import BaseModel, Field, validator

import mlflow
import mlflow.keras
from google.cloud import monitoring_v3
from google.cloud import logging as gcp_logging

# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configuration TensorFlow pour stabilit√©
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # R√©duire les logs TensorFlow
tf.get_logger().setLevel('ERROR')

# Configuration MLflow avec d√©sactivation pour production
ENVIRONMENT = os.getenv("ENVIRONMENT", "development")
if ENVIRONMENT == "production":
    MLFLOW_AVAILABLE = False
    logger.info("üö´ MLflow d√©sactiv√© en production")
else:
    try:
        os.environ.setdefault("MLFLOW_TRACKING_URI", "http://localhost:5000")
        mlflow.set_experiment("air_paradis_sentiment_production")
        MLFLOW_AVAILABLE = True
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è MLflow non disponible: {e}")
        MLFLOW_AVAILABLE = False

# Configuration Google Cloud
PROJECT_ID = os.getenv("GOOGLE_CLOUD_PROJECT", "air-paradis-sentiment")
REGION = os.getenv("GOOGLE_CLOUD_REGION", "europe-west1")

# Variables globales pour le mod√®le
model = None
tokenizer = None
config = None
MAX_SEQUENCE_LENGTH = 50

# M√©triques de monitoring
error_count = 0
last_errors = []

class CompatibleTokenizer:
    """Tokenizer compatible cr√©√© √† partir d'un word_index extract√©"""
    
    def __init__(self, word_index, num_words=None):
        self.word_index = word_index
        self.num_words = num_words or len(word_index)
        self.word_counts = None
        self.word_docs = None
        self.index_word = {v: k for k, v in word_index.items()}
    
    def texts_to_sequences(self, texts):
        """Convertit les textes en s√©quences d'entiers"""
        sequences = []
        for text in texts:
            if isinstance(text, str):
                words = text.lower().split()
            else:
                words = text
            
            sequence = []
            for word in words:
                index = self.word_index.get(word, 0)  # 0 pour mots inconnus
                if self.num_words is None or index < self.num_words:
                    sequence.append(index)
                else:
                    sequence.append(0)  # Mot hors vocabulaire
            sequences.append(sequence)
        return sequences

class ModelManager:
    """Gestionnaire du mod√®le de sentiment - Compatible TensorFlow 2.13 avec gestion tokenizers"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.config = None
        self.model_path = "models/best_advanced_model_BiLSTM_Word2Vec.h5"
        self.tokenizer_path = "models/best_advanced_model_tokenizer.pickle"
        self.config_path = "models/best_advanced_model_config.pickle"
        self.is_dummy_model = False
        self.tokenizer_type = "dummy"  # Valeur par d√©faut pour √©viter les erreurs
        
    async def load_model(self):
        """Charge le mod√®le et ses composants avec gestion des incompatibilit√©s TF 2.13"""
        try:
            logger.info("üîß Chargement du mod√®le BiLSTM + Word2Vec...")
            
            # Configuration TensorFlow pour mod√®les avec batch_shape
            tf.keras.backend.clear_session()
            
            # Chargement du mod√®le TensorFlow avec gestion des erreurs batch_shape
            await self._load_tensorflow_model()
            
            # Chargement du tokenizer avec gestion des incompatibilit√©s
            await self._load_tokenizer_compatible()
            
            # Chargement de la config
            await self._load_config()
            
            # Test du mod√®le
            await self._test_model()
            
        except Exception as e:
            logger.error(f"‚ùå Erreur critique lors du chargement: {str(e)}")
            # Fallback complet vers mod√®le factice
            self.model = self._create_compatible_dummy_model()
            self.tokenizer = self._create_dummy_tokenizer()
            self.config = {"max_sequence_length": MAX_SEQUENCE_LENGTH}
            self.is_dummy_model = True
            self.tokenizer_type = "dummy"
            logger.info("üé≠ Fallback vers mod√®le factice complet")
    
    async def _load_tensorflow_model(self):
        """Charge le mod√®le TensorFlow avec gestion batch_shape"""
        if os.path.exists(self.model_path):
            try:
                # Tentative de chargement normal
                self.model = load_model(self.model_path, compile=False)
                logger.info("‚úÖ Mod√®le TensorFlow charg√© avec succ√®s")
                self.is_dummy_model = False
                
                # Recompiler le mod√®le pour s'assurer de la compatibilit√©
                self.model.compile(
                    optimizer='adam',
                    loss='binary_crossentropy',
                    metrics=['accuracy']
                )
                
            except Exception as e:
                if "batch_shape" in str(e) or "InputLayer" in str(e):
                    logger.warning(f"‚ö†Ô∏è Erreur batch_shape d√©tect√©e: {e}")
                    logger.info("üîÑ Tentative de chargement avec custom_objects...")
                    
                    try:
                        # Chargement avec gestion personnalis√©e des objets
                        custom_objects = {
                            'InputLayer': tf.keras.layers.InputLayer
                        }
                        self.model = load_model(
                            self.model_path, 
                            custom_objects=custom_objects,
                            compile=False
                        )
                        logger.info("‚úÖ Mod√®le charg√© avec custom_objects")
                        self.is_dummy_model = False
                        
                    except Exception as e2:
                        logger.error(f"‚ùå √âchec du chargement avec custom_objects: {e2}")
                        logger.info("üé≠ Basculement vers mod√®le factice")
                        self.model = self._create_compatible_dummy_model()
                        self.is_dummy_model = True
                else:
                    logger.error(f"‚ùå Erreur de chargement du mod√®le: {e}")
                    self.model = self._create_compatible_dummy_model()
                    self.is_dummy_model = True
        else:
            logger.warning(f"‚ö†Ô∏è Mod√®le non trouv√©: {self.model_path}")
            logger.info("üé≠ Cr√©ation d'un mod√®le factice compatible")
            self.model = self._create_compatible_dummy_model()
            self.is_dummy_model = True
    
    async def _load_tokenizer_compatible(self):
        """Charge le tokenizer avec gestion des incompatibilit√©s TensorFlow 2.13"""
        if os.path.exists(self.tokenizer_path):
            try:
                with open(self.tokenizer_path, 'rb') as f:
                    tokenizer_data = pickle.load(f)
                
                # Cas 1: Tokenizer Keras standard compatible
                if hasattr(tokenizer_data, 'texts_to_sequences') and hasattr(tokenizer_data, 'word_index'):
                    try:
                        # Test de fonctionnalit√©
                        test_sequences = tokenizer_data.texts_to_sequences(["test"])
                        self.tokenizer = tokenizer_data
                        self.tokenizer_type = "keras_native"
                        logger.info("‚úÖ Tokenizer Keras natif charg√©")
                        return
                    except Exception as e:
                        if "keras.src.legacy" in str(e):
                            logger.warning("‚ö†Ô∏è Tokenizer incompatible TF 2.13 (keras.src.legacy)")
                            # Extraire le word_index pour cr√©er un tokenizer compatible
                            word_index = getattr(tokenizer_data, 'word_index', {})
                            num_words = getattr(tokenizer_data, 'num_words', None)
                            self.tokenizer = CompatibleTokenizer(word_index, num_words)
                            self.tokenizer_type = "compatible_extracted"
                            logger.info("‚úÖ Tokenizer compatible cr√©√© √† partir du word_index")
                            return
                        else:
                            raise e
                
                # Cas 2: Format dict avec word_index
                elif isinstance(tokenizer_data, dict) and 'word_index' in tokenizer_data:
                    self.tokenizer = CompatibleTokenizer(
                        tokenizer_data['word_index'],
                        tokenizer_data.get('num_words', None)
                    )
                    self.tokenizer_type = "dict_format"
                    logger.info("‚úÖ Tokenizer cr√©√© √† partir du format dict")
                    return
                
                # Cas 3: Tentative d'extraction du word_index
                else:
                    logger.warning("‚ö†Ô∏è Format de tokenizer non reconnu, tentative d'extraction...")
                    word_index = {}
                    
                    # Essayer diff√©rentes m√©thodes d'extraction
                    if hasattr(tokenizer_data, '__dict__'):
                        attrs = tokenizer_data.__dict__
                        if 'word_index' in attrs:
                            word_index = attrs['word_index']
                        elif '_word_index' in attrs:
                            word_index = attrs['_word_index']
                    
                    if word_index:
                        self.tokenizer = CompatibleTokenizer(word_index)
                        self.tokenizer_type = "extracted"
                        logger.info("‚úÖ Tokenizer cr√©√© par extraction du word_index")
                        return
                    else:
                        raise ValueError("Impossible d'extraire le word_index")
                        
            except Exception as e:
                logger.error(f"‚ùå Erreur chargement tokenizer: {e}")
                logger.info("üé≠ Cr√©ation d'un tokenizer factice")
                self.tokenizer = self._create_dummy_tokenizer()
                self.tokenizer_type = "dummy"
        else:
            logger.warning(f"‚ö†Ô∏è Tokenizer non trouv√©: {self.tokenizer_path}")
            self.tokenizer = self._create_dummy_tokenizer()
            self.tokenizer_type = "dummy"
    
    async def _load_config(self):
        """Charge la configuration"""
        if os.path.exists(self.config_path):
            try:
                with open(self.config_path, 'rb') as f:
                    self.config = pickle.load(f)
                logger.info("‚úÖ Configuration charg√©e")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur chargement config: {e}")
                self.config = {"max_sequence_length": MAX_SEQUENCE_LENGTH}
        else:
            logger.warning("‚ö†Ô∏è Configuration par d√©faut")
            self.config = {
                "max_sequence_length": MAX_SEQUENCE_LENGTH,
                "vocab_size": 10000,
                "embedding_dim": 300
            }
    
    def _create_compatible_dummy_model(self):
        """Cr√©e un mod√®le factice compatible avec TensorFlow 2.13"""
        try:
            from tensorflow.keras.models import Sequential
            from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, SpatialDropout1D
            
            # Mod√®le simple sans batch_shape pour √©viter les conflits
            model = Sequential([
                Embedding(10000, 300, input_length=MAX_SEQUENCE_LENGTH),
                SpatialDropout1D(0.2),
                Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)),
                Dense(64, activation='relu'),
                Dense(1, activation='sigmoid')
            ])
            
            model.compile(
                optimizer='adam',
                loss='binary_crossentropy',
                metrics=['accuracy']
            )
            
            # Initialiser avec des donn√©es factices
            dummy_X = np.random.randint(0, 1000, (32, MAX_SEQUENCE_LENGTH))
            dummy_y = np.random.randint(0, 2, (32, 1))
            model.fit(dummy_X, dummy_y, epochs=1, verbose=0)
            
            logger.info("‚úÖ Mod√®le factice compatible cr√©√©")
            return model
            
        except Exception as e:
            logger.error(f"‚ùå Erreur cr√©ation mod√®le factice: {e}")
            # Fallback vers pr√©dicteur ultra-simple
            class UltraSimplePredictor:
                def predict(self, x, batch_size=None):
                    # Analyse simple bas√©e sur les mots
                    batch_size = x.shape[0] if hasattr(x, 'shape') else 1
                    return np.random.uniform(0.3, 0.9, (batch_size, 1))
            
            return UltraSimplePredictor()
    
    def _create_dummy_tokenizer(self):
        """Cr√©e un tokenizer factice pour les tests"""
        word_index = {
            'love': 1, 'great': 2, 'excellent': 3, 'amazing': 4, 'best': 5,
            'hate': 6, 'terrible': 7, 'worst': 8, 'awful': 9, 'bad': 10,
            'airline': 11, 'flight': 12, 'service': 13, 'crew': 14, 'staff': 15,
            'i': 16, 'the': 17, 'is': 18, 'was': 19, 'this': 20,
            'air': 21, 'paradis': 22, 'good': 23, 'ok': 24, 'fine': 25,
            'nice': 26, 'comfortable': 27, 'friendly': 28, 'professional': 29,
            'disappointing': 30, 'delayed': 31, 'cancelled': 32, 'rude': 33
        }
        
        return CompatibleTokenizer(word_index, 10000)
    
    async def _test_model(self):
        """Test du mod√®le avec gestion des erreurs"""
        try:
            test_text = "I love this airline, great service!"
            prediction = await self.predict(test_text)
            model_type = "factice" if self.is_dummy_model else "r√©el"
            logger.info(f"‚úÖ Test du mod√®le {model_type} r√©ussi: '{test_text}' -> {prediction}")
            logger.info(f"üìä Tokenizer: {self.tokenizer_type}")
        except Exception as e:
            logger.error(f"‚ùå √âchec du test du mod√®le: {str(e)}")
            raise
    
    async def predict(self, text: str) -> Dict:
        """Pr√©diction de sentiment avec gestion batch_shape et tokenizers incompatibles"""
        try:
            if not self.model or not self.tokenizer:
                raise ValueError("Mod√®le ou tokenizer non charg√©")
            
            # Pr√©processing du texte
            sequences = self.tokenizer.texts_to_sequences([text])
            max_len = self.config.get("max_sequence_length", MAX_SEQUENCE_LENGTH)
            padded = pad_sequences(sequences, maxlen=max_len)
            
            # Pr√©diction avec gestion des diff√©rents types de mod√®les
            try:
                if hasattr(self.model, 'predict'):
                    # Pr√©diction avec gestion batch_size
                    prediction_prob = self.model.predict(padded, batch_size=1, verbose=0)[0][0]
                else:
                    # Fallback pour mod√®les ultra-simples
                    prediction_prob = 0.8  # Valeur par d√©faut positive
                    
            except Exception as pred_error:
                logger.warning(f"‚ö†Ô∏è Erreur de pr√©diction: {pred_error}")
                # Analyse heuristique simple en cas d'√©chec
                positive_words = ['love', 'great', 'excellent', 'amazing', 'best', 'good', 'nice']
                negative_words = ['hate', 'terrible', 'worst', 'awful', 'bad', 'disappointing']
                
                text_lower = text.lower()
                positive_count = sum(1 for word in positive_words if word in text_lower)
                negative_count = sum(1 for word in negative_words if word in text_lower)
                
                if positive_count > negative_count:
                    prediction_prob = 0.75
                elif negative_count > positive_count:
                    prediction_prob = 0.25
                else:
                    prediction_prob = 0.5
            
            prediction_class = int(prediction_prob > 0.5)
            
            # Mapping des r√©sultats
            sentiment_mapping = {0: "negative", 1: "positive"}
            sentiment = sentiment_mapping[prediction_class]
            
            confidence = float(prediction_prob) if prediction_class == 1 else float(1 - prediction_prob)
            
            result = {
                "text": text,
                "sentiment": sentiment,
                "confidence": round(confidence, 4),
                "probability": round(float(prediction_prob), 4),
                "model": f"BiLSTM_Word2Vec{' (factice)' if self.is_dummy_model else ''}",
                "tokenizer_type": self.tokenizer_type,
                "timestamp": datetime.utcnow().isoformat()
            }
            
            # Log MLflow avec gestion d'erreurs
            if MLFLOW_AVAILABLE:
                try:
                    with mlflow.start_run(run_name=f"prediction_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"):
                        mlflow.log_param("text_length", len(text))
                        mlflow.log_metric("confidence", confidence)
                        mlflow.log_metric("probability", float(prediction_prob))
                        mlflow.log_param("sentiment", sentiment)
                        mlflow.log_param("is_dummy_model", self.is_dummy_model)
                        mlflow.log_param("tokenizer_type", self.tokenizer_type)
                except Exception as mlflow_error:
                    logger.warning(f"‚ö†Ô∏è Erreur MLflow: {mlflow_error}")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la pr√©diction: {str(e)}")
            raise

# Instance globale du gestionnaire de mod√®le
model_manager = ModelManager()

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Gestionnaire du cycle de vie de l'application"""
    # Startup
    logger.info("üöÄ D√©marrage de l'API Air Paradis Sentiment Analysis")
    logger.info(f"üîß TensorFlow version: {tf.__version__}")
    logger.info(f"üìä MLflow disponible: {MLFLOW_AVAILABLE}")
    await model_manager.load_model()
    logger.info("‚úÖ API pr√™te √† recevoir des requ√™tes")
    yield
    # Shutdown
    logger.info("üõë Arr√™t de l'API")

# Mod√®les Pydantic v1
class SentimentRequest(BaseModel):
    text: str = Field(..., min_length=1, max_length=500, description="Texte √† analyser")
    user_id: Optional[str] = Field(None, description="ID de l'utilisateur (optionnel)")
    
    @validator('text')
    def validate_text(cls, v):
        if not v or not v.strip():
            raise ValueError('Le texte ne peut pas √™tre vide')
        return v.strip()

class SentimentResponse(BaseModel):
    text: str
    sentiment: str
    confidence: float
    probability: float
    model: str
    tokenizer_type: str
    timestamp: str
    request_id: Optional[str] = None

class FeedbackRequest(BaseModel):
    text: str
    predicted_sentiment: str
    actual_sentiment: str
    is_correct: bool
    user_id: Optional[str] = None

class HealthResponse(BaseModel):
    status: str
    model_loaded: bool
    model_type: str
    tokenizer_type: str
    tensorflow_version: str
    mlflow_available: bool
    timestamp: str
    version: str = "1.0.0"

# Initialisation FastAPI
app = FastAPI(
    title="Air Paradis Sentiment Analysis API",
    description="API d'analyse de sentiment pour les tweets - Compatible TensorFlow 2.13 + tokenizers",
    version="1.0.0",
    lifespan=lifespan
)

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Monitoring Google Cloud (fonctions inchang√©es)
async def send_error_to_monitoring(error_message: str, text: str):
    """Envoie une erreur au monitoring Google Cloud"""
    try:
        global error_count, last_errors
        error_count += 1
        current_time = datetime.utcnow()
        
        last_errors.append({
            "timestamp": current_time,
            "message": error_message,
            "text": text
        })
        
        five_minutes_ago = current_time.timestamp() - 300
        last_errors = [
            err for err in last_errors 
            if err["timestamp"].timestamp() > five_minutes_ago
        ]
        
        if len(last_errors) >= 3:
            await send_alert_email()
            
        logger.warning(f"‚ö†Ô∏è Erreur envoy√©e au monitoring: {error_message}")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'envoi au monitoring: {str(e)}")

async def send_alert_email():
    """Envoie une alerte par email via Google Cloud"""
    try:
        logger.critical("üö® ALERTE: 3 erreurs d√©tect√©es en 5 minutes!")
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'envoi d'alerte: {str(e)}")

# Routes de l'API

@app.get("/", response_model=Dict)
async def root():
    """Route racine de l'API"""
    return {
        "message": "Air Paradis Sentiment Analysis API",
        "version": "1.0.0",
        "tensorflow_version": tf.__version__,
        "model": f"BiLSTM_Word2Vec{' (factice)' if model_manager.is_dummy_model else ''}",
        "tokenizer_type": model_manager.tokenizer_type,
        "mlflow_available": MLFLOW_AVAILABLE,
        "endpoints": {
            "predict": "/predict",
            "health": "/health",
            "feedback": "/feedback",
            "docs": "/docs"
        }
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """V√©rification de l'√©tat de l'API"""
    model_loaded = model_manager.model is not None and model_manager.tokenizer is not None
    model_type = "factice" if model_manager.is_dummy_model else "r√©el"
    
    return HealthResponse(
        status="healthy" if model_loaded else "unhealthy",
        model_loaded=model_loaded,
        model_type=model_type,
        tokenizer_type=model_manager.tokenizer_type,
        tensorflow_version=tf.__version__,
        mlflow_available=MLFLOW_AVAILABLE,
        timestamp=datetime.utcnow().isoformat()
    )

@app.post("/predict", response_model=SentimentResponse)
async def predict_sentiment(
    request: SentimentRequest,
    background_tasks: BackgroundTasks
):
    """Pr√©diction de sentiment pour un texte"""
    try:
        # Validation
        if not request.text.strip():
            raise HTTPException(status_code=400, detail="Le texte ne peut pas √™tre vide")
        
        # Pr√©diction
        result = await model_manager.predict(request.text)
        
        # G√©n√©ration d'un ID de requ√™te
        request_id = f"req_{datetime.utcnow().strftime('%Y%m%d_%H%M%S_%f')}"
        result["request_id"] = request_id
        
        logger.info(f"‚úÖ Pr√©diction r√©ussie: {request.text[:50]}... -> {result['sentiment']}")
        
        return SentimentResponse(**result)
        
    except HTTPException:
        raise
    except Exception as e:
        error_msg = f"Erreur lors de la pr√©diction: {str(e)}"
        logger.error(f"‚ùå {error_msg}")
        
        background_tasks.add_task(send_error_to_monitoring, error_msg, request.text)
        
        raise HTTPException(status_code=500, detail="Erreur interne du serveur")

@app.post("/feedback")
async def submit_feedback(
    feedback: FeedbackRequest,
    background_tasks: BackgroundTasks
):
    """Soumission de feedback utilisateur"""
    try:
        logger.info(f"üìù Feedback re√ßu: {feedback.text[:50]}... - Correct: {feedback.is_correct}")
        
        if MLFLOW_AVAILABLE:
            try:
                with mlflow.start_run(run_name=f"feedback_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"):
                    mlflow.log_param("text", feedback.text)
                    mlflow.log_param("predicted_sentiment", feedback.predicted_sentiment)
                    mlflow.log_param("actual_sentiment", feedback.actual_sentiment)
                    mlflow.log_metric("is_correct", 1 if feedback.is_correct else 0)
                    mlflow.log_param("user_id", feedback.user_id or "anonymous")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur MLflow feedback: {e}")
        
        if not feedback.is_correct:
            error_msg = f"Pr√©diction incorrecte: pr√©dit '{feedback.predicted_sentiment}', r√©el '{feedback.actual_sentiment}'"
            background_tasks.add_task(send_error_to_monitoring, error_msg, feedback.text)
        
        return {
            "message": "Feedback enregistr√© avec succ√®s",
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'enregistrement du feedback: {str(e)}")
        raise HTTPException(status_code=500, detail="Erreur lors de l'enregistrement du feedback")

@app.get("/metrics")
async def get_metrics():
    """M√©triques de l'API"""
    return {
        "total_errors": error_count,
        "recent_errors_5min": len(last_errors),
        "model_loaded": model_manager.model is not None,
        "model_type": "factice" if model_manager.is_dummy_model else "r√©el",
        "tokenizer_type": model_manager.tokenizer_type,
        "tensorflow_version": tf.__version__,
        "mlflow_available": MLFLOW_AVAILABLE,
        "timestamp": datetime.utcnow().isoformat()
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
